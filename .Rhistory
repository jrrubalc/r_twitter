mydata = read.table("rockabilly.txt")
setwd(~/r_code/r_twitter)
setwd("~/r_code/r_twitter")
mydata = read.table("rockabilly.txt")
View(mydata)
mydata[1]
mydata[1]
t(mydata)
mydata=t(mydata)
mydata=[,1]
mydata
View(mydata)
mydata[,1]
mydata[,2]
mydata = read.table("rockabilly.txt")
mydata[1,]
mydata[1,0]
mydata[1,1]
mydata=t(mydata)
mydata = read.table("rockabilly.txt")
mydata=t(mydata)
mydata[1,]
mydata[,1]
mydata[,1]
mydata = read.table("rockabilly.txt", header = TRUE)
mydata=t(mydata)
mydata[,1]
mydata$musician[,1]
mydata = read.table("rockabilly.txt", header = TRUE)
mydata$musician
list = t(mydata$musician)
list
list
list[,1]
mydata=t(mydata)
mydata
mydata[,1]
mydata[,1] =asd
asd = mydata[,1]
asd
mydata
mydata$musician[[1]]
mydata[[1]]
mydata[[2]]
mydata = read.table("rockabilly.txt", header = TRUE)
mydata=t(mydata)
mydata[[2]]
sunArtist = read.table("rockabilly.txt", header = TRUE)
sunArtist=t(sunArtist)
r_stats<- searchTwitter(mydata[[1]], n=1500)
source('~/r_code/r_twitter/twitter_api.r')
length(sunArtist)
for (i in 1:length(sunArtist)) {
print(sunArtist[[i]])
}
library("twitteR")
library("wordcloud")
library("tm")
source("credentials.r")
#to get your consumerKey and consumerSecret see the twitteR documentation for instructions
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
sunArtist = read.table("rockabilly.txt", header = TRUE)
sunArtist=t(sunArtist)
for (i in 1:length(sunArtist)) {
tweets<- searchTwitter(mydata[[1]], n=1500)
text = laply(tweets, function(t) t$getText() )
}
install.packages("laply")
library("laply")
install.packages("plyr")
library("plyr")
text = laply(tweets, function(t) t$getText() )
tweets
for (i in 1:length(sunArtist)) {
tweets<- searchTwitter(mydata[[1]], n=5)
}
text = laply(tweets, function(t) t$getText() )
text
append(text, text)
allText = []
append(zzz, text)
tweets
x <- vector(mode="numeric", length=0)
test <- vector(mode="string", length=0)
test <- vector(length=0)
append(test,tweets)
append(test,tweets)
append(test,tweets)
test
testq <- vector(length=0)
append(testq,tweets)
append(testq,tweets)
append(testq,tweets)
testq = c(testq,tweets)
testq
testq = c(testq,tweets)
testq = c(testq,tweets)
testq
allTweets = vector(length=0)
for (i in 1:length(sunArtist)) {
current_tweets<- searchTwitter(mydata[[1]], n=15)
allTweets = c(allTweets,current_tweets)
}
all_tweets
all_Tweets
allTweets
text = laply(allTweets, function(t) t$getText() )
text
View(sunArtist)
for (i in 1:length(sunArtist)) {
current_tweets<- searchTwitter(mydata[[i]], n=15)
allTweets = c(allTweets,current_tweets)
}
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
install.packages("plyr")
install.packages("stringr")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
pos.words = scan(file.path(dataDir, 'opinion-lexicon-English', 'positive-words.txt'), what='character', comment.char=';')
pos.words = scan(file.path('opinion-lexicon-English', 'positive-words.txt'), what='character', comment.char=';')
pos.words = scan('positive-words.txt', what='character', comment.char=';')
pos.words = scan('positive-words.txt', what='character', comment.char=';')
neg.words = scan('negative-words.txt', what='character', comment.char=';')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores
install.packages("textcat")
textcat("hello")
library("textcat")
textcat("hello")
textcat("kjhgfdsa")
textcat("Elvis Presley Photo book With a poster 1977 printed japan F/S  free shipping https://t.co/0FH178hMd")
textcat("ELVIS PRESLEY  - LOVING YOU VOL 1 / LOVING YOU VOL II - EPA 1-1515  https://t.co/FqPxw1cpZN https://t.co/jzM2ryYVcV")
textcat("lkjhg kjhg tre mnbvcx")
textcat("63263526363265390- ++_")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
textcat("خلصت ديسكوجرافي johnny cash مرتين ������")
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
text
textcat(text)
textcat(text[1])
textcat(text[7])
textcat(text[8])
textcat(text[10])
text[10]
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
install.packages("validUTF8")
install.packages("validUTF8")
library("validUTF8")
install.packages("validUTF8")
install.packages("validUTF8")
install.packages("tau")
library("tau")
is.utf8("the bird man")
is.utf8("Big's News: Tarantula species named for Johnny Cash - An expansive, decade-long survey of tarantulas in the Uni... https://t.co/ihw4ofdfIC")
is.utf8("خلصت ديسكوجرافي")
source("score_sentiment.r")
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
library("textcat")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
is.utf8("Big's News: Tarantula species named for Johnny Cash - An expansive, decade-long survey of tarantulas in the Uni... https://t.co/ihw4ofdfIC")
is.utf8("at the coon")
is.utf8("Big's News: Tarantula species named for Johnny Cash - An expansive, decade-long survey of tarantulas in the Uni... https://t.co/ihw4ofdfIC")
fixEncoding("hello")
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source('~/r_code/r_twitter/twitter_api.r')
text[1]
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
function (x, n = 3L, split = "[[:space:][:punct:][:digit:]]+",
tolower = TRUE, marker = "_", words = NULL, lower = 0L, method = c("ngram",
"string", "prefix", "suffix"), recursive = FALSE, persistent = FALSE,
useBytes = FALSE, perl = TRUE, verbose = FALSE, decreasing = FALSE)
{
if (is.list(x)) {
if (recursive) {
if (persistent) {
for (z in x[-length(x)]) textcnt(z, n, split,
tolower, marker, words, lower, method, recursive,
persistent, perl, useBytes, verbose)
x <- x[length(x)]
persistent <- FALSE
}
else return(lapply(x, textcnt, n, split, tolower,
marker, words, lower, method, recursive, persistent,
useBytes, perl, verbose))
}
}
else {
if (is.null(x))
return(x)
x <- list(x)
}
n <- as.integer(n)
if (n < 0L)
stop("'n' invalid value")
if (n < 1L)
return(NULL)
method <- match.arg(method)
if (!is.null(split))
x <- if (!is.null(formals(strsplit)$useBytes))
lapply(lapply(x, strsplit, split, perl = perl, useBytes = useBytes),
unlist)
else lapply(lapply(x, strsplit, split, perl = perl),
unlist)
if (!useBytes && tolower)
x <- lapply(x, tolower)
if (!is.null(words))
x <- .Call(R_copyTruncate, x, words)
if (method == "ngram") {
if (length(grep(marker, unlist(x, use.names = FALSE),
useBytes = TRUE)))
stop("'marker' contained in 'x'")
if (marker == "\002") {
m <- paste(rep(marker, n - 1L), collapse = "")
x <- lapply(x, function(x) gsub("$(?<!^)", m, x,
perl = TRUE, useBytes = useBytes))
}
x <- lapply(x, function(x) gsub("^(?!$)|$(?<!^)", marker,
x, perl = TRUE, useBytes = useBytes))
x <- .Call(R_utf8CountNgram, x, n, lower, verbose, persistent,
useBytes)
if (length(x) && marker == "\002") {
i <- grep("^\002{2,}$", names(x), useBytes = useBytes)
text <- "iðŸ’™you"
iconv(text, "UTF8", "ASCII", sub="")
if (is.list(x)) {
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores
scores[1]
scores[[1]
]
source('~/r_code/r_twitter/twitter_api.r')
allTweets
library(plyr)
tweets.df = ldply(allTweets, function(t) t$toDataFrame())
write.csv(tweets.df, file = "newfile.csv")
source('~/r_code/r_twitter/twitter_api.r')
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
allTweets.df = ldply(allTweets, function(t) t$toDataFrame())
write.csv(allTweets.df, file = "twitter_data.csv")
print("Reading in search phrases.")
sunArtist = read.table("rockabilly.txt", header = TRUE)
sunArtist=t(sunArtist)
allTweets = vector(length=0)
print("Searching Twitter")
print("Performing sentiment analysis.")
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
print("Saving data to CSV.")
allTweets.df = ldply(allTweets, function(t) t$toDataFrame())
write.csv(allTweets.df, file = "twitter_data.csv")
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source('~/r_code/r_twitter/twitter_api.r')
source('~/r_code/r_twitter/twitter_api.r')
print("Performing sentiment analysis.")
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
print("Saving data to CSV.")
allTweets.df = ldply(allTweets, function(t) t$toDataFrame())
write.csv(allTweets.df, file = "twitter_data.csv")
searchTerms = read.table("rockabilly.txt", header = TRUE)
View(searchTerms)
searchTerms=t(searchTerms)
allTweets = vector(length=0)
print("Searching Twitter")
for (i in 1:length(sunArtist)) {
current_tweets<- searchTwitter(mydata[[i]], n=500)
allTweets = c(allTweets,current_tweets)
}
for (i in 1:length(sunArtist)) {
current_tweets<- searchTwitter(searchTerms[[i]], n=500)
allTweets = c(allTweets,current_tweets)
}
searchTerms[[i]]
print("Performing sentiment analysis.")
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
print("Saving data to CSV.")
allTweets.df = ldply(allTweets, function(t) t$toDataFrame())
write.csv(allTweets.df, file = "twitter_data.csv")
View(allTweets.df)
View(searchTerms)
print("Searching Twitter")
for (i in 1:length(searchTerms)) {
current_tweets<- searchTwitter(searchTerms[[i]], n=5)
allTweets = c(allTweets,current_tweets)
}
print("Reading in search phrases.")
searchTerms = read.table("rockabilly.txt", header = TRUE)
searchTerms=t(searchTerms)
allTweets = vector(length=0)
print("Searching Twitter")
for (i in 1:length(searchTerms)) {
current_tweets<- searchTwitter(searchTerms[[i]], n=5)
allTweets = c(allTweets,current_tweets)
}
searchTerms[[i]]
searchTerms[[1]]
searchTerms = read.table("rockabilly.txt", header = TRUE)
searchTerms=t(searchTerms)
allTweets = vector(length=0)
print("Searching Twitter")
for (i in 1:length(searchTerms)) {
current_tweets<- searchTwitter(searchTerms[[i]], n=5)
allTweets = c(allTweets,current_tweets)
}
length(searchTerms)
print("Performing sentiment analysis.")
text = laply(allTweets, function(t) t$getText() )
scores = score.sentiment(text, pos.words, neg.words, .progress='text')
source("score_sentiment.r")
source('~/r_code/r_twitter/twitter_api.r')
View(allTweets.df)
